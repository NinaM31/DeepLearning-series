{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "Previously we were introduced to a step cost function, in this notebook I'll talk about **Step**, **Sigmoid**, **Softmax** and **Relu** activation functions, they decide how active a perceptron is.\n",
    "\n",
    "![IMG_0672](https://user-images.githubusercontent.com/57009004/138087476-42259db4-8828-4ee8-8f8b-66c9071b7a97.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Function\n",
    "\n",
    "* The perceptron outputs a 1 or a 0 ( active not active ).\n",
    "* Used for binary classification.\n",
    "* The function is discrete.\n",
    "\n",
    "#### In Math:\n",
    "$$f(x_i)= \\begin{cases}\n",
    "    1& \\text{if } x_i \\geq 0\\\\\n",
    "    0& \\text{otherwise} \n",
    "\\end{cases}$$ \n",
    "\n",
    "#### In Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    if x >= 0: return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Logistic\n",
    "Sigmoid function refers to an S-shaped curve, it can be \\[ Logistic, Hyperbolic, Archtangent\\], Logistic function is the most common.\n",
    "* The inputs should be normalized at Â± 4 because all values will be squashed between 0 & 1.\n",
    "* Used when we have 2 classes ( Cat, dog ).\n",
    "* Subject to vanishing gradient ( Bad thing ).\n",
    "* The function is continuous.\n",
    "* Perceptron outputs a value between 0 & 1 ( strength of activity )\n",
    "\n",
    "#### In Math:\n",
    "$$f(x_i)= \\frac{1}{(1 + e^{-x_i})}$$ \n",
    "\n",
    "#### In Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/ ( 1 + np.exp(-x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "* The inputs need to be normalized.\n",
    "* used for multiple classes ( Cat, dog, bird ).\n",
    "* For each class the output will be of range 0 & 1 , but all add up to 1.\n",
    "* The function is continuous.\n",
    "\n",
    "#### In Math\n",
    "$$f(x_i)=\\frac{e^{x_i}}{\\sum_{j=0}^{k} e^{x_j}}, \\text{where k is number of classes}$$\n",
    "#### In Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    expX = np.exp(x)\n",
    "    return np.divide( expX, expX.sum() ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "* Inputs do not need to be normalized.\n",
    "* Solves vanishing gradients but subject to dying ReLU ( Bad thing ).\n",
    "* Fast to compute & favored when network has many layers.\n",
    "* Perceptron outpts is a range between 0 & x\n",
    "* The function is continuous.\n",
    "\n",
    "#### In Math\n",
    "$$f(x_i) = \\text{max( } 0, x\\text{ )}$$\n",
    "#### In Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
